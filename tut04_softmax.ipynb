{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a4050d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Bonus. Exploring the Softmax Function\n",
    "\n",
    "In lecture and tutorial, we explored how the sigmoid function $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$ is used in single-class and multi-class classification problems. Yet, in the case of multi-class classification problems, a common alternative is to use the softmax function instead.\n",
    "\n",
    "The softmax function is defined as follows, where $K$ is the number of classes:\n",
    "\n",
    "$$\\text{softmax}(\\boldsymbol{z}) = \\begin{bmatrix}\n",
    "\\dfrac{e^{z_1}}{\\sum_{j=1}^K e^{z_j}} \\\\\n",
    "\\dfrac{e^{z_2}}{\\sum_{j=1}^K e^{z_j}} \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{e^{z_K}}{\\sum_{j=1}^K e^{z_j}} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "In Pytorch, these two functions are implemented in `torch.sigmoid` and `torch.softmax` respectively.\n",
    "\n",
    "**Your task**:\n",
    "1. Investigate the relationship between the softmax function and the sigmoid function when $K = 2$. To showcase your understanding, complete the function `mysoftmax` that computes the softmax of tensor `z` **only relying on the `torch.sigmoid` function**  (and a minimal amount of other operations).\n",
    "1. Work out the derivative of the softmax function. Again, to showcase your understanding, complete the function `mysoftmax_grad` that computes the derivative of the softmax of tensor `z`. You may use the `torch.softmax` function, but you should use your own formula to compute the derivative instead of using `torch.autograd.functional.jacobian()`. (Hint: Recall that $\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$. You should expect to get something very similar!)\n",
    "1. In the case of multi-class classification (when $K > 2$), under what scenarios would you consider using the softmax function instead of the sigmoid function, or vice versa?\n",
    "\n",
    "**Submission:** Send me a screenshot of your `mysoftmax` + `mysoftmax_grad` implementation and the writeup before/during the tutorial (for bonus EXP)!\n",
    "\n",
    "You may check out [Pytorch documentation](https://pytorch.org/docs/stable/torch.html) for the functions manipulating a `torch.tensor`. Work out the equation on paper first before coding, and do some googling if you are stuck.\n",
    "\n",
    "P.S. If no one solves all three tasks, I will still give out bonus EXP to those who solved at least 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9734fb23",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915cb23a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def mysoftmax(z):\n",
    "    \"\"\"\n",
    "    Computes the softmax of a two-dimensional tensor z across dimension 1.\n",
    "    Same as the problem sets, your solution must not involve any iteration.\n",
    "    \"\"\"\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ce4385",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax of z:\n",
      " tensor([[0.6214, 0.3786],\n",
      "        [0.2190, 0.7810],\n",
      "        [0.1938, 0.8062],\n",
      "        [0.7362, 0.2638],\n",
      "        [0.1739, 0.8261]])\n",
      "Softmax of z (implemented using sigmoid):\n",
      " tensor([[ 1.6579,  1.1623],\n",
      "        [-0.7738,  0.4975],\n",
      "        [-0.0625,  1.3629],\n",
      "        [ 1.1550,  0.1289],\n",
      "        [-0.3080,  1.2504]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Output does not match",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftmax of z (implemented using sigmoid):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, z_test)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m z_test\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m z_correct\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape does not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(torch\u001b[38;5;241m.\u001b[39misclose(z_test, z_correct), dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)), \\\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput does not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Output does not match"
     ]
    }
   ],
   "source": [
    "# Sample test case.\n",
    "\n",
    "torch.manual_seed(2109)\n",
    "z = torch.randn(5, 2)\n",
    "\n",
    "z_correct = z.clone().detach()\n",
    "z_correct = torch.softmax(z_correct, dim=1)\n",
    "print(\"Softmax of z:\\n\", z_correct)\n",
    "\n",
    "z_test = z.clone().detach()\n",
    "z_test = mysoftmax(z_test)\n",
    "print(\"Softmax of z (implemented using sigmoid):\\n\", z_test)\n",
    "\n",
    "assert z_test.shape == z_correct.shape, \"Output shape does not match\"\n",
    "assert torch.all(torch.isclose(z_test, z_correct), dim=(0,1)), \\\n",
    "    \"Output does not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13fa86",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Output does not match",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m z_eval_test \u001b[38;5;241m=\u001b[39m z_eval\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     10\u001b[0m z_eval_test \u001b[38;5;241m=\u001b[39m mysoftmax(z_eval_test)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(torch\u001b[38;5;241m.\u001b[39misclose(z_eval_test, z_eval_correct), dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)), \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput does not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLarge test case passed. Congratulations!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Output does not match"
     ]
    }
   ],
   "source": [
    "# Large test case. Please make sure you pass this.\n",
    "\n",
    "torch.manual_seed(3264)\n",
    "z_eval = torch.randn(100, 2)\n",
    "\n",
    "z_eval_correct = z_eval.clone().detach()\n",
    "z_eval_correct = torch.softmax(z_eval_correct, dim=1)\n",
    "\n",
    "z_eval_test = z_eval.clone().detach()\n",
    "z_eval_test = mysoftmax(z_eval_test)\n",
    "\n",
    "assert torch.all(torch.isclose(z_eval_test, z_eval_correct), dim=(0,1)), \\\n",
    "    \"Output does not match\"\n",
    "\n",
    "print(\"Large test case passed. Congratulations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "446b56",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def mysoftmax_grad(z):\n",
    "    \"\"\"\n",
    "    Computes the softmax derivative of a two-dimensional tensor z across dimension 1.\n",
    "    It should return a three-dimensional tensor.\n",
    "    The (i,j,k)-th entry of the output tensor should contain the derivative of softmax(z_i)_j with respect to (z_i)_k.\n",
    "    Same as the problem sets, your solution must not involve any iteration.\n",
    "    \"\"\"\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "962cd1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax derivative of z:\n",
      " tensor([[[ 0.2420, -0.2115, -0.0305],\n",
      "         [-0.2115,  0.2301, -0.0186],\n",
      "         [-0.0305, -0.0186,  0.0491]],\n",
      "\n",
      "        [[ 0.1892, -0.0367, -0.1525],\n",
      "         [-0.0367,  0.1238, -0.0871],\n",
      "         [-0.1525, -0.0871,  0.2396]]])\n",
      "Softmax derivative of z (from the formula):\n",
      " tensor([[ 1.6579,  1.1623, -0.7738],\n",
      "        [ 0.4975, -0.0625,  1.3629]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Output shape does not match",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m z_test \u001b[38;5;241m=\u001b[39m mysoftmax_grad(z_test)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftmax derivative of z (from the formula):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, z_test)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m z_test\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m z_correct\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape does not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(torch\u001b[38;5;241m.\u001b[39misclose(z_test, z_correct), dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)), \\\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput does not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Output shape does not match"
     ]
    }
   ],
   "source": [
    "# Sample test case.\n",
    "\n",
    "torch.manual_seed(2109)\n",
    "z = torch.randn(2, 3)\n",
    "\n",
    "softmax_fn = lambda z: torch.softmax(z, dim=1)\n",
    "\n",
    "z_correct = z.clone().detach().requires_grad_(True)\n",
    "z_correct = torch.autograd.functional.jacobian(softmax_fn, z_correct)\n",
    "z_correct = z_correct.diagonal(dim1=0, dim2=2).permute((2, 0, 1))\n",
    "print(\"Softmax derivative of z:\\n\", z_correct)\n",
    "\n",
    "z_test = z.clone().detach()\n",
    "z_test = mysoftmax_grad(z_test)\n",
    "print(\"Softmax derivative of z (from the formula):\\n\", z_test)\n",
    "\n",
    "assert z_test.shape == z_correct.shape, \"Output shape does not match\"\n",
    "assert torch.all(torch.isclose(z_test, z_correct), dim=(0,1,2)), \\\n",
    "    \"Output does not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f945cd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Output shape does not match",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m z_test \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     13\u001b[0m z_test \u001b[38;5;241m=\u001b[39m mysoftmax_grad(z_test)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m z_test\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m z_correct\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape does not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(torch\u001b[38;5;241m.\u001b[39misclose(z_test, z_correct), dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)), \\\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput does not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLarge test case passed. Congratulations!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Output shape does not match"
     ]
    }
   ],
   "source": [
    "# Large test case. Please make sure you pass this.\n",
    "\n",
    "torch.manual_seed(3264)\n",
    "z = torch.randn(50, 10)\n",
    "\n",
    "softmax_fn = lambda z: torch.softmax(z, dim=1)\n",
    "\n",
    "z_correct = z.clone().detach().requires_grad_(True)\n",
    "z_correct = torch.autograd.functional.jacobian(softmax_fn, z_correct)\n",
    "z_correct = z_correct.diagonal(dim1=0, dim2=2).permute((2, 0, 1))\n",
    "\n",
    "z_test = z.clone().detach()\n",
    "z_test = mysoftmax_grad(z_test)\n",
    "\n",
    "assert z_test.shape == z_correct.shape, \"Output shape does not match\"\n",
    "assert torch.all(torch.isclose(z_test, z_correct), dim=(0,1,2)), \\\n",
    "    \"Output does not match\"\n",
    "\n",
    "print(\"Large test case passed. Congratulations!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}