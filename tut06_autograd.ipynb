{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a691091",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Bonus. Behind Pytorch Autograd\n",
    "\n",
    "When we implement Neural networks in Pytorch, we only need to consider the forward pass and the backward pass is handled automatically by autograd (the automatic differentiation engine in Pytorch).\n",
    "\n",
    "As mentioned during lecture, the autograd engine saves a graph, called the autograd graph, based on the operations done during forward propagation (but in **reverse order**). This graph is then traversed during the call to `backward()`.\n",
    "\n",
    "Have you ever wondered what's inside the autograd graph?\n",
    "\n",
    "In fact, each `torch.Tensor` has an attribute called `grad_fn`, which is an [`torch.autograd.graph.Node`](https://pytorch.org/docs/stable/autograd.html#autograd-graph) object (i.e. a node in the autograd graph). In this bonus lab, we will try to explore what's inside the node and play around with that.\n",
    "\n",
    "*Note:* You'll need the `graphviz` package for this task. If you are using a local installation of Python, see [this stackoverflow post](https://stackoverflow.com/questions/35064304/runtimeerror-make-sure-the-graphviz-executables-are-on-your-systems-path-aft) for instructions.\n",
    "\n",
    "**Your task:** The following 3 tasks guides you to explore the autograd graph. Complete the tasks in sequence.\n",
    "\n",
    "**Submission**: Submit your writeup to Task 1 and your implementation to Tasks 2 - 3 before/during the tutorial for extra EXP.\n",
    "\n",
    "If no one solves all 3 tasks, I'll still give out bonus EXP for those who solved at least 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0333ec",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d65af",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Task 1: A Simple Hook for the Backward Pass\n",
    "\n",
    "Notice that the `torch.autograd.graph.Node` class contains a method called `register_hook`, which allows us to hook the backward propagation process!\n",
    "\n",
    "Let's try to create a very simple linear model and call `backward()` on it. Also, we register a hook on the layer so that our hook function gets called during backward propagation.\n",
    "\n",
    "**Question**: What does the `input` and `output` parameters of `hook_fn` represent in general? Try to see if you can derive the expressions to predict the arguments `input` and `output` based on $\\boldsymbol{x}$, $\\boldsymbol{W}$, $\\boldsymbol{y}$ and $\\boldsymbol{\\hat{y}}$.\n",
    "\n",
    "*There is no coding involved in this task. However, you can add print statements to verify your claims.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f8cd6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hooked! Input: (None, tensor([[-0.4500, -1.1500],\n",
      "        [-0.9000, -2.3000]])), Output: (tensor([[-0.4500, -1.1500]]),)\n"
     ]
    }
   ],
   "source": [
    "def hook_fn(input, output):\n",
    "    print(f\"Hooked! Input: {input}, Output: {output}\")\n",
    "\n",
    "class VerySimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VerySimpleNet, self).__init__()\n",
    "        self.linear = nn.Linear(2, 2, bias=False)\n",
    "        self.linear.weight = torch.nn.Parameter(torch.Tensor([[0.15,0.20],[0.25,0.30]]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        # We register a hook on the autograd node attached to x.\n",
    "        x.grad_fn.register_hook(hook_fn)\n",
    "        return x\n",
    "\n",
    "model = VerySimpleNet()\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Forward propagation\n",
    "x = torch.Tensor([[1, 2]])\n",
    "y = torch.Tensor([[1, 2]])\n",
    "y_pred = model(x)\n",
    "output = loss(y, y_pred)\n",
    "\n",
    "# Backward propagation\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3f1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Task 2: Creating the Autograd Graph\n",
    "\n",
    "Now, we understand that Pytorch autograd traverses the autograd graph node by node and calculates their gradients. But wait... where is the graph?\n",
    "\n",
    "As we all know, graphs contain edges. Therefore, the autograd graph definitely stores edges between nodes to tell what gradients the engine should calculate next.\n",
    "\n",
    "The edges are hidden somewhere within the [`torch.autograd.graph.Node`](https://pytorch.org/docs/stable/autograd.html#autograd-graph) class. Read the documentation and find out where the edges are stored.\n",
    "\n",
    "To demonstrate your understanding, we have already written a boilerplate code that generates a visualization of the autograd graph, but the critical logic has not been implemented yet (which is, finding the edges of the autograd graph). Complete the function `add_nodes` so that it enumerates all neighbours of the current node in the autograd graph, and traverse those neighbours recursively.\n",
    "\n",
    "*Hint 1*: Find the list of available attibutes in the documention page above. Do you find anything suspicious?  \n",
    "*Hint 2*: Focus on the type of the attribute. Make sure you can locate the `Node` objects containing the neighbour nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ac39b79",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_autograd_graph(loss):\n",
    "    \"\"\" Produces a visualization of PyTorch autograd graph \"\"\"    \n",
    "\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "    visited = set()\n",
    "\n",
    "    \"\"\" Helper functions \"\"\"\n",
    "\n",
    "    def size_to_str(size):\n",
    "        return '('+(', ').join(['%d'% v for v in size])+')'\n",
    "\n",
    "    def create_parameter_node(var, size):\n",
    "        dot.node(str(id(var)), size_to_str(size), fillcolor='lightblue')\n",
    "\n",
    "    def create_func_node(var):\n",
    "        dot.node(str(id(var)), str(type(var).__name__))\n",
    "\n",
    "    def create_edge(u, v):\n",
    "        dot.edge(str(id(u)), str(id(v)))\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var in visited:\n",
    "            return\n",
    "        visited.add(var)\n",
    "\n",
    "        if hasattr(var, 'variable'):\n",
    "            create_parameter_node(var, var.variable.size())\n",
    "        else:\n",
    "            create_func_node(var)\n",
    "\n",
    "        # TODO: Complete the implementation.\n",
    "        pass\n",
    "\n",
    "    add_nodes(loss.grad_fn)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2518cf7c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Incorrect number of internal nodes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Visualize the gradient graph\u001b[39;00m\n\u001b[1;32m     27\u001b[0m dot \u001b[38;5;241m=\u001b[39m visualize_autograd_graph(output)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBackward\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x, dot\u001b[38;5;241m.\u001b[39mbody))) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m, \\\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect number of internal nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightblue\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x, dot\u001b[38;5;241m.\u001b[39mbody))) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m, \\\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect number of tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m->\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x, dot\u001b[38;5;241m.\u001b[39mbody))) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m, \\\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect number of edges\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Incorrect number of internal nodes"
     ]
    }
   ],
   "source": [
    "# Sample test case\n",
    "\n",
    "# Sample neural network with 2 linear layers\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "model = TwoLayerNet(1000, 100, 10)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Forward propagation\n",
    "x = torch.randn(64, 1000)  # Random input data\n",
    "y = torch.randn(64, 10)  # Random target data\n",
    "y_pred = model(x) \n",
    "output = loss(y, y_pred)\n",
    "\n",
    "# Visualize the gradient graph\n",
    "dot = visualize_autograd_graph(output)\n",
    "\n",
    "assert len(list(filter(lambda x: 'Backward' in x, dot.body))) == 6, \\\n",
    "    \"Incorrect number of internal nodes\"\n",
    "assert len(list(filter(lambda x: 'lightblue' in x, dot.body))) == 4, \\\n",
    "    \"Incorrect number of tensors\"\n",
    "assert len(list(filter(lambda x: '->' in x, dot.body))) == 9, \\\n",
    "    \"Incorrect number of edges\"\n",
    "\n",
    "print(\"Sample test case passed, congratulations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "114e37",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"147pt\" height=\"29pt\"\n",
       " viewBox=\"0.00 0.00 147.00 29.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 25)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-25 143,-25 143,4 -4,4\"/>\n",
       "<!-- 139916507301296 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139916507301296</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"139,-21 0,-21 0,0 139,0 139,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"69.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\">MseLossBackward0</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f40d9b74880>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this to visualize the autograd graph\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c774",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Task 3: Visited Memory?\n",
    "\n",
    "Notice that in the implementation of `add_nodes` above, we have used visited memory to avoid visiting an identical object `var` multiple times. We could delete those lines and the resulting implementation would still pass the sample test case above.\n",
    "\n",
    "However, this visited memory is **necessary for correctness**. Your task is to demonstrate this.\n",
    "\n",
    "**Task**: Create a (minimal) neural network so that `visualize_autograd_graph` will yield different visualizations **with** or **without** visited memory.\n",
    "\n",
    "*Hint*: You may want to recall what we discussed in tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b01f3d98",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"147pt\" height=\"29pt\"\n",
       " viewBox=\"0.00 0.00 147.00 29.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 25)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-25 143,-25 143,4 -4,4\"/>\n",
       "<!-- 139916508805584 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139916508805584</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"139,-21 0,-21 0,0 139,0 139,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"69.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\">MseLossBackward0</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f40d9ce3430>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HackNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HackNet, self).__init__()\n",
    "        # TODO: Initialize your neural network here\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward function\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# TODO: You may change the following code,\n",
    "#       e.g. editing the dimensions of the input/target.\n",
    "model = HackNet()\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "x = torch.randn(64, 10)\n",
    "y = torch.randn(64, 10)\n",
    "y_pred = model(x)\n",
    "output = loss(y, y_pred)\n",
    "\n",
    "visualize_autograd_graph(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {},
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
